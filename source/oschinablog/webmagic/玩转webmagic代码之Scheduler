<h2>玩转webmagic代码之Scheduler</h2><div class="BlogContent">
 <p>webmagic上线之后，因为灵活性很强，得到了一些爬虫老手的欢迎，但是对于新手来说可能稍微摸不着头脑，我的需求是这样子，什么模块化，什么灵活性，但是看了半天，我也不知道怎么解决我的问题啊？</p> 
 <p>这里先谈谈Scheduler，不单关乎框架，更多是一些爬虫通用的思想，希望对大家有帮助。</p> 
 <span id="OSC_h2_1"></span> 
 <h2>为什么要有Scheduler</h2> 
 <p>其实Scheduler并非webmagic独创，在scrapy以及其他成熟爬虫中都有类似模块。Scheduler管理了所有待抓取的url，单个爬虫自己是无法控制要抓取什么的，抓什么都由Scheduler决定。</p> 
 <p>这样子最大的好处就是，爬虫本身没有状态，给一个url，处理一个，非常容易进行水平扩展(就是加线程、或者加机器)，而且即使单台爬虫宕机，也不会有什么损失。这跟我们在应用开发中，所说的&quot;服务无状态&quot;的思想是很像的。而相反，如果在单个爬虫线程内部，循环甚至递归的进行抓取，那么这部分工作是无法扩展的，而且宕机之后恢复会很困难。</p> 
 <pre class="brush: java; auto-links: false;">public interface Scheduler {

    public void push(Request request, Task task);

    public Request poll(Task task);

}</pre> 
 <p>webmagic里的Scheduler只有两个接口，一个放入url，一个取出url。</p> 
 <span id="OSC_h2_2"></span> 
 <h2>玩转Scheduler</h2> 
 <span id="OSC_h3_3"></span> 
 <h3>层级关系及上下文信息</h3> 
 <p>我们这里举一个较复杂的例子。例如，我们要从<a href="http://www.ip138.com/post/" rel="nofollow">http://www.ip138.com/post/</a>上抓取全国的邮编地址，最后我们想要得到一个树状结构的结果，这个结果包括<strong>省 市 县 村/街道 邮编</strong>。这里有两个需求：一个是优先抓最终页面，一个是要带上所有前面页面的信息。如果随便手写一个爬虫，可能我们就会用递归的形式写了，那么在webmagic里如何做呢？</p> 
 <p>从0.2.1起，webmagic的<code>Request</code>，也就是保存待抓取url的对象，有两个大的改动：</p> 
 <p>一个是支持优先级，这样子要深度优先还是广度优先，都可以通过给不同层次设置不同值完成。</p> 
 <p>二是可以在<code>Request</code>里附加额外信息<code>request.putExtra(key,value)</code>，这个额外信息会带到下次页面抓取中去。</p> 
 <p>于是，我们可以通过给最终页面增加高优先级，达到优先抓取的目的；同时可以把之前抓取的信息保存到<code>Request</code>里去，在最终结果中，附加上前面页面的信息。</p> 
 <p>最终代码在<a href="https://github.com/code4craft/webmagic/blob/master/webmagic-samples/src/main/java/us/codecraft/webmagic/samples/scheduler/ZipCodePageProcessor.java" rel="nofollow">这里</a>，当然，其实这个例子里，最终页面是包含“省”、“市”信息的，这里只是讨论附加信息的可能性。</p> 
 <pre class="brush: java; auto-links: false;">public class ZipCodePageProcessor implements PageProcessor {

    private Site site = Site.me().setCharset(&quot;gb2312&quot;)
            .setSleepTime(100).addStartUrl(&quot;http://www.ip138.com/post/&quot;);

    @Override
    public void process(Page page) {
        if (page.getUrl().toString().equals(&quot;http://www.ip138.com/post/&quot;)) {
            processCountry(page);
        } else if (page.getUrl().regex(&quot;http://www\\.ip138\\.com/post/\\w+[/]?$&quot;).toString() != null) {
            processProvince(page);
        } else {
            processDistrict(page);
        }

    }

    private void processCountry(Page page) {
        List&lt;String&gt; provinces = page.getHtml().xpath(&quot;//*[@id=\&quot;newAlexa\&quot;]/table/tbody/tr/td&quot;).all();
        for (String province : provinces) {
            String link = xpath(&quot;//@href&quot;).select(province);
            String title = xpath(&quot;/text()&quot;).select(province);
            Request request = new Request(link).setPriority(0).putExtra(&quot;province&quot;, title);
            page.addTargetRequest(request);
        }
    }

    private void processProvince(Page page) {
        //这里仅靠xpath没法精准定位，所以使用正则作为筛选，不符合正则的会被过滤掉
        List&lt;String&gt; districts = page.getHtml().xpath(&quot;//body/table/tbody/tr/td&quot;).regex(&quot;.*http://www\\.ip138\\.com/post/\\w+/\\w+.*&quot;).all();
        for (String district : districts) {
            String link = xpath(&quot;//@href&quot;).select(district);
            String title = xpath(&quot;/text()&quot;).select(district);
            Request request = new Request(link).setPriority(1).putExtra(&quot;province&quot;, page.getRequest().getExtra(&quot;province&quot;)).putExtra(&quot;district&quot;, title);
            page.addTargetRequest(request);
        }
    }

    private void processDistrict(Page page) {
        String province = page.getRequest().getExtra(&quot;province&quot;).toString();
        String district = page.getRequest().getExtra(&quot;district&quot;).toString();
        List&lt;String&gt; counties = page.getHtml().xpath(&quot;//body/table/tbody/tr&quot;).regex(&quot;.*&lt;td&gt;\\d+&lt;/td&gt;.*&quot;).all();
        String regex = &quot;&lt;td[^&lt;&gt;]*&gt;([^&lt;&gt;]+)&lt;/td&gt;&lt;td[^&lt;&gt;]*&gt;([^&lt;&gt;]+)&lt;/td&gt;&lt;td[^&lt;&gt;]*&gt;([^&lt;&gt;]+)&lt;/td&gt;&lt;td[^&lt;&gt;]*&gt;([^&lt;&gt;]+)&lt;/td&gt;&quot;;
        for (String county : counties) {
            String county0 = regex(regex, 1).select(county);
            String county1 = regex(regex, 2).select(county);
            String zipCode = regex(regex, 3).select(county);
            page.putField(&quot;result&quot;, StringUtils.join(new String[]{province, district,
                    county0, county1, zipCode}, &quot;\t&quot;));
        }
        List&lt;String&gt; links = page.getHtml().links().regex(&quot;http://www\\.ip138\\.com/post/\\w+/\\w+&quot;).all();
        for (String link : links) {
            page.addTargetRequest(new Request(link).setPriority(2).putExtra(&quot;province&quot;, province).putExtra(&quot;district&quot;, district));
        }

    }

    @Override
    public Site getSite() {
        return site;
    }

    public static void main(String[] args) {
        Spider.create(new ZipCodePageProcessor()).scheduler(new PriorityScheduler()).run();
    }
}</pre> 
 <p>这段代码略复杂，因为我们其实进行了了3种页面的抽取，论单个页面，还是挺简单的:)</p> 
 <p>同样的，我们可以实现一个最多抓取n层的爬虫。通过在request.extra里增加一个&quot;层数&quot;的概念即可做到，而Scheduler只需做少量定制：</p> 
 <pre class="brush: java; auto-links: false;">public class LevelLimitScheduler extends PriorityScheduler {

    private int levelLimit = 3;

    public LevelLimitScheduler(int levelLimit) {
        this.levelLimit = levelLimit;
    }

    @Override
    public synchronized void push(Request request, Task task) {
        if (((Integer) request.getExtra(&quot;_level&quot;)) &lt;= levelLimit) {
            super.push(request, task);
        }
    }
}</pre> 
 <span id="OSC_h3_4"></span> 
 <h3>按照指定URL查询</h3> 
 <p>例如我想要抓取百度某些关键词查询的结果，这个需求再简单不过了，你可以先新建一个Scheduler，将想要查询的URL全部放入Scheduler之后，再启动Spider即可：</p> 
 <pre class="brush: java; auto-links: false;">PriorityScheduler scheduler = new PriorityScheduler();
Spider spider = Spider.create(new ZipCodePageProcessor()).scheduler(scheduler);
scheduler.push(new Request(&quot;http://www.baidu.com/s?wd=webmagic&quot;),spider);
//这里webmagic是关键词
...//其他地址
spider.run();</pre> 
 <span id="OSC_h3_5"></span> 
 <h3>定期轮询</h3> 
 <p>有一类需求是，定期检查页面是否更新，如果更新，则抓取最新数据。这里包括两