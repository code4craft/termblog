<h2>webmagic的设计机制及原理-如何开发一个Java爬虫</h2><div class="BlogContent">
 <p><img src="http://static.oschina.net/uploads/space/2013/1110/200709_oP1e_190591.jpg" alt="image" /></p> 
 <blockquote> 
  <p>此文章是webmagic 0.1.0版的设计手册，后续版本的入门及用户手册请看这里：<a href="https://github.com/code4craft/webmagic/blob/master/user-manual.md" rel="nofollow">https://github.com/code4craft/webmagic/blob/master/user-manual.md</a></p> 
 </blockquote> 
 <p>之前就有网友在博客里留言，觉得webmagic的实现比较有意思，想要借此研究一下爬虫。最近终于集中精力，花了三天时间，终于写完了这篇文章。之前垂直爬虫写了一年多，webmagic框架写了一个多月，这方面倒是有一些心得，希望对读者有帮助。</p> 
 <span id="OSC_h2_1"></span> 
 <h2>webmagic的目标</h2> 
 <p>一般来说，一个爬虫包括几个部分：</p> 
 <ul> 
  <li><p>页面下载</p> <p>页面下载是一个爬虫的基础。下载页面之后才能进行其他后续操作。</p> </li> 
  <li><p>链接提取</p> <p>一般爬虫都会有一些初始的种子URL，但是这些URL对于爬虫是远远不够的。爬虫在爬页面的时候，需要不断发现新的链接。</p> </li> 
  <li><p>URL管理</p> <p>最基础的URL管理，就是对已经爬过的URL和没有爬的URL做区分，防止重复爬取。</p> </li> 
  <li><p>内容分析和持久化</p> <p>一般来说，我们最终需要的都不是原始的HTML页面。我们需要对爬到的页面进行分析，转化成结构化的数据，并存储下来。</p> </li> 
 </ul> 
 <p>不同的爬虫，对这几部分的要求是不一样的。</p> 
 <p>对于通用型的爬虫，例如搜索引擎蜘蛛，需要指对互联网大部分网页无差别进行抓取。这时候难点就在于页面下载和链接管理上–如果要高效的抓取更多页面，就必须进行更快的下载；同时随着链接数量的增多，需要考虑如果对大规模的链接进行去重和调度，就成了一个很大的问题。一般这些问题都会在大公司有专门的团队去解决，比如这里有一篇来自淘宝的<a href="http://www.searchtb.com/2011/07/%E5%BF%AB%E9%80%9F%E6%9E%84%E5%BB%BA%E5%AE%9E%E6%97%B6%E6%8A%93%E5%8F%96%E9%9B%86%E7%BE%A4.html?spm=0.0.0.0.hHzGxv" rel="nofollow">快速构建实时抓取集群</a>。对Java来说，如果你要研究通用爬虫，那么可以看一下<a href="http://www.oschina.net/p/heritrix" rel="nofollow"><strong>heritrix</strong></a>或者<a href="http://www.oschina.net/p/nutch" rel="nofollow"><strong>nutch</strong></a>。</p> 
 <p>而垂直类型的爬虫要解决的问题则不一样，比如想要爬取一些网站的新闻、博客信息，一般抓取数量要求不是很大，难点则在于如何高效的定制一个爬虫，可以精确的抽取出网页的内容，并保存成结构化的数据。这方面需求很多，webmagic就是为了解决这个目的而开发的。</p> 
 <p>使用Java语言开发爬虫是比较复杂的。虽然Java有很强大的页面下载、HTML分析工具，但是每个都有不小的学习成本，而且这些工具本身都不是专门为爬虫而生，使用起来也没有那么顺手。我曾经有一年的时间都在开发爬虫，重复的开发让人头痛。Java还有一个比较成熟的框架<a href="https://code.google.com/p/crawler4j/" rel="nofollow"><strong>crawler4j</strong></a>，但是它是为通用爬虫而设计的，扩展性差一些，满足不了我的业务需要。我也有过自己开发框架的念头，但是终归觉得抽象的不是很好。直到发现python的爬虫框架<a href="http://scrapy.org/" rel="nofollow"><strong>scrapy</strong></a>，它将爬虫的生命周期拆分的非常清晰，我参照它进行了模块划分，并用Java的方式去实现了它，于是就有了webmagic。</p> 
 <p>代码已经托管到github，地址是<a href="https://github.com/code4craft/webmagic" rel="nofollow">https://github.com/code4craft/webmagic</a>，Javadoc：<a href="http://code4craft.github.io/webmagic/docs/" rel="nofollow">http://code4craft.github.io/webmagic/docs/</a></p> 
 <p>webmagic的实现还参考了另一个Java爬虫<a href="https://gitcafe.com/laiweiwei/Spiderman" rel="nofollow"><strong>SpiderMan</strong></a>。SpiderMan是一个全栈式的Java爬虫，它的设计思想跟webmagic稍有不同，它希望将Java语言的实现隔离，仅仅让用户通过配置就完成一个垂直爬虫。理论上，SpiderMan功能更强大，很多功能已经内置，而webmagic则比较灵活，适合熟悉Java语法的开发者，可以比较非常方便的进行扩展和二次开发。</p> 
 <span id="OSC_h2_2"></span> 
 <h2>webmagic的模块划分</h2> 
 <p>webmagic目前的核心代码都在<strong>webmagic-core</strong>中，<strong>webmagic-samples</strong>里有一些定制爬虫的例子，可以作为参考。而<strong>webmagic-plugin</strong>目前还不完善，后期准备加入一些常用的功能。下面主要介绍webmagic-core的内容。</p> 
 <p>前面说到，webmagic参考了scrapy的模块划分，分为Spider(整个爬虫的调度框架)、Downloader(页面下载)、PageProcessor(链接提取和页面分析)、Scheduler(URL管理)、Pipeline(离线分析和持久化)几部分。只不过scrapy通过middleware实现扩展，而webmagic则通过定义这几个接口，并将其不同的实现注入主框架类Spider来实现扩展。</p> 
 <p><img src="/action/blog/img_proxy?url=http%3A%2F%2Fcode4craft.github.io%2Fimages%2Fposts%2Fwebmagic.png" alt="image" /></p> 
 <span id="OSC_h3_3"></span> 
 <h3>Spider类-核心调度</h3> 
 <p>Spider是爬虫的入口类，Spider的接口调用采用了链式的API设计，其他功能全部通过接口注入Spider实现，下面是启动一个比较复杂的Spider的例子。</p> 
 <pre class="brush: java; auto-links: false;">Spider.create(sinaBlogProcessor)
.scheduler(new FileCacheQueueScheduler(&quot;/data/temp/webmagic/cache/&quot;))
.pipeline(new FilePipeline())
.thread(10).run();</pre> 
 <p>Spider的核心处理流程非常简单，代码如下：</p> 
 <pre class="brush: java; auto-links: false;">private void processRequest(Request request) {
    Page page = downloader.download(request, this);
    if (page == null) {
        sleep(site.getSleepTime());
        return;
    }
    pageProcessor.process(page);
    addRequest(page);
    for (Pipeline pipeline : pipelines) {
        pipeline.process(page, this);
    }
    sleep(site.getSleepTime());
}</pre> 
 <span id="OSC_h3_4"></span> 
 <h3>Downloader-页面下载</h3> 
 <p>页面下载是一切爬虫的开始。</p> 
 <p>大部分爬虫都是通过模拟http请求，接收并分析响应来完成。这方面，JDK自带的<strong>HttpURLConnection</strong>可以满足最简单的需要，而<strong>Apache HttpClient</strong>(4.0后整合到HttpCompenent项目中)则是开发复杂爬虫的不二之选。它支持自定义HTTP头(对于爬虫比较有用的就是User-agent、cookie等)、自动redirect、连接复用、cookie保留、设置代理等诸多强大的功能。</p> 
 <p>webmagic使用了HttpClient 4.2，并封装到了<strong>HttpClientDownloader</strong>。学习HttpClient的使用对于构建高性能爬虫是非常有帮助的，官方的<a href="http://hc.apache.org/httpcomponents-client-ga/tutorial/html/" rel="nofollow">Tutorial</a>就是很好的学习资料。目前webmagic对HttpClient的使用仍在初步阶段，不过对于一般抓取任务，已经够用了。</p> 
 <p>下面是一个使用HttpClient最简单的例子：</p> 
 <pre class="brush: java; auto-links: false;">HttpClient httpClient = new DefaultHttpClient();
HttpGet httpGet = new HttpGet(&quot;http://youhost/xxx&quot;);
HttpResponse httpResponse = httpClient.execute(httpGet);
System.out.println(EntityUtils.toString(httpResponse.getEntity().getContent()));</pre> 
 <p>对于一些Javascript动态加载的网页，仅仅使用http模拟下载工具，并不能取到页面的内容。这方面的思路有两种：一种是抽丝剥茧，分析js的逻辑，再用爬虫去重现它(比如在网页中提取关键数据，再用这些数据去构造Ajax请求，最后直接从响应体获取想要的数据)； <br />另一种就是：内置一个浏览器，直接获取最后加载完的页面。这方面，js可以使用<strong>PhantomJS</strong>，它内部集成了webkit。而Java可以使用<strong>Selenium</strong>，这是一个非常强大的浏览器模拟工具。考虑以后将它整理成一个独立的Downloader，集成到webmagic中去。</p> 
 <p>一般没有必要去扩展Downloader。</p> 
 <span id="OSC_h3_5"></span> 
 <h3>PageProcessor-页面分析及链接抽取</h3> 
 <p>这里说的页面分析主要指HTML页面的分析。页面分析可以说是垂直爬虫最复杂的一部分，在webmagic里，PageProcessor是定制爬虫的核心。通过编写一个实现PageProcessor接口的类，就可以定制一个自己的爬虫。</p> 
 <p>页面抽取最基本的方式是使用正则表达式。正则表达式好处是非常通用，解析文本的功能也很强大。但是正则表达式最大的问题是，不能真正对HTML进行语法级别的解析，没有办法处理关系到HTML结构的情况(例如处理标签嵌套)。例如，我想要抽取一个&lt;div&gt;里的内容，可以这样写：“&lt;div&gt;(.*?)&lt;/div&gt;“。但是如果这个div内部还包含几个子div，这个时候使用正则表达式就会将子div的”&lt;/div&gt;“作为终止符截取。为了解决这个问题，我们就需要进行HTML的分析。</p> 
 <p>HTML分析是一个比较复杂的工作，Java世界主要有几款比较方便的分析工具：</p> 
 <span id="OSC_h4_6"></span> 
 <h4>Jsoup</h4> 
 <p>Jsoup是一个集强大和便利于一体的HTML解析工具。它方便的地方是，可以用于支持用jquery中css selector的方式选取元素，这对于熟悉js的开发者来说基本没有学习成本。</p> 
 <pre class="brush: java; auto-links: false;">String content = &quot;blabla&quot;;
Document doc = JSoup.parse(content);
Elements links = doc.select(&quot;a[href]&quot;);</pre> 
 <p>Jsoup还支持白名单过滤机制，对于网站防止XSS攻击也是很好的。</p> 
 <span id="OSC_h4_7"></span> 
 <h4>HtmlParser</h4> 
 <p>HtmlParser的功能比较完备，也挺灵活，但谈不上方便。这个项目很久没有维护了，最新版本是2.1。HtmlParser的核心元素是Node，对应一个HTML标签，支持getChildren()等树状遍历方式。HtmlParser另外一个核心元素是NodeFilter，通过实现NodeFilter接口，可以对页面元素进行筛选。这里有一篇HtmlParser的使用文章：<a href="https://www.ibm.com/developerworks/cn/opensource/os-cn-crawler/" rel="nofollow">使用 HttpClient 和 HtmlParser 实现简易爬虫</a>。</p> 
 <span id="OSC_h4_8"></span> 
 <h4>Apache tika</h4> 
 <p>tika是专为抽取而生的工具，还支持PDF、Zip甚至是Java Class。使用tika分析HTML，需要自己定义一个抽取内容的Handler并继承<code>org.xml.sax.helpers.DefaultHandler</code>，解析方式就是xml标准的方式。crawler4j中就使用了tika作为解析工具。SAX这种流式的解析方式对于分析大文件很有用，我个人倒是认为对于解析html意义不是很大。</p> 
 <pre class="brush: java; auto-links: false;">InputStream inputStream = null;
HtmlParser htmlParser = new HtmlParser();
htmlParser.parse(new ByteArrayInputStream(page.getContentData()), 
contentHandler, metadata, new ParseContext());</pre> 
 <span id="OSC_h4_9"></span> 
 <h4>HtmlCleaner与XPath</h4> 
 <p>HtmlCleaner最大的优点是：支持XPath的方式选取元素。XPath是一门在XML中查找信息的语言，也可以用于抽取HTML元素。XPath与CSS Selector大部分功能都是重合的，但是CSS Selector专门针对HTML，写法更简洁，而XPath则是通用的标准，可以精确到属性值。XPath有一定的学习成本，但是对经常需要编写爬虫的人来说，这点投入绝对是值得的。</p> 
 <p>学习XPath可以参考w3school的<a href="http://www.w3school.com.cn/xpath/" rel="nofollow">XPath 教程</a>。下面是使用HtmlCleaner和xpath进行抽取的一段代码：</p> 
 <pre class="brush: java; auto-links: false;">HtmlCleaner htmlCleaner = new HtmlCleaner();
TagNode tagNode = htmlCleaner.clean(text);
Object[] objects = tagNode.evaluateXPath(&quot;xpathStr&quot;);</pre> 
 <span id="OSC_h4_10"></span> 
 <h4>几个工具的对比</h4> 
 <p>在这里评价这些工具的主要标准是“方便”。就拿抽取页面所有链接这一基本任务来说，几种代码分别如下：</p> 
 <p>XPath:</p> 
 <pre class="brush: java; auto-links: false;">tagNode.evaluateXPath(&quot;//a/@href&quot;)</pre> 
 <p>CSS Selector:</p> 
 <pre class="brush: java; auto-links: false;">//使用类似js的实现
$(&quot;a[href]&quot;).attr(&quot;href&quot;)</pre> 
 <p>HtmlParser：</p> 
 <pre class="brush: java; auto-links: false;">Parser p = new Parser(value);
NodeFilter aFilter = new TagNameFilter(&quot;a&quot;);
NodeList nodes = p.extractAllNodesThatMatch(aFilter);
for (int i = 0; i &lt; nodes.size(); i++) {
    Node eachNode = nodes.elementAt(i);
    if (eachNode instanceof LinkTag) {
        LinkTag linkTag = (LinkTag) eachNode;
        System.out.println(linkTag.extractLink());
    }
}</pre> 
 <p>XPath是最简单的，可以精确选取到href属性值；而CSS Selector则次之，可以选取到HTML标签，属性值需要调用函数去获取；而HtmlParser和SAX则需要手动写程序去处理标签了，比较麻烦。</p> 
 <span id="OSC_h4_11"></span> 
 <h4>webmagic的Selector</h4> 
 <p><strong>Selector</strong>是webmagic为了简化页面抽取开发的独立模块，是整个项目中我最得意的部分。这里整合了CSS Selector、XPath和正则表达式，并可以进行链式的抽取，很容易就实现强大的功能。即使你使用自己开发的爬虫工具，webmagic的Selector仍然值得一试。</p> 
 <p>例如，我已经下载了一个页面，现在要抽取某个区域的所有包含&quot;blog&quot;的链接，我可以这样写：</p> 
 <pre class="brush: java; auto-links: false;">//content是用别的爬虫工具抽取到的正文
String content = &quot;blabla&quot;;
List&lt;String&gt; links = Html.create(content)
.$(&quot;div.title&quot;)  //css 选择，Java里虽然很少有$符号出现，不过貌似$作为方法名是合法的
.xpath(&quot;//@href&quot;)  //提取链接
.regex(&quot;.*blog.*&quot;) //正则匹配过滤
.all(); //转换为string列表</pre> 
 <p>另外，webmagic的抓取链接需要显示的调用<code>Page.addTargetRequests()</code>去添加，这也是为了灵活性考虑的(很多时候，下一步的URL不是单纯的页面href链接，可能会根据页面模块进行抽取，甚至可能是自己拼凑出来的)。</p> 
 <p>补充一个有意思的话题，就是对于页面正文的自动抽取。相信用过Evernote Clearly都会对其自动抽取正文的技术印象深刻。这个技术又叫<strong>Readability</strong>，webmagic对readability有一个粗略的实现<strong>SmartContentSelector</strong>，用的是P标签密度计算的方法，在测试oschina博客时有不错的效果。</p> 
 <span id="OSC_h3_12"></span> 
 <h3>Scheduler-URL管理</h3> 
 <p>URL管理的问题可大可小。对于小规模的抓取，URL管理是很简单的。我们只需要将待抓取URL和已抓取URL分开保存，并进行去重即可。使用JDK内置的集合类型Set、List或者Queue都可以满足需要。如果我们要进行多线程抓取，则可以选择线程安全的容器，例如LinkedBlockingQueue以及ConcurrentHashMap。</p> 
 <p>因为小规模的URL管理非常简单，很多框架都并不将其抽象为一个模块，而是直接融入到代码中。但是实际上，抽象出Scheduler模块，会使得框架的解耦程度上升一个档次，并非常容易进行横向扩展，这也是我从scrapy中学到的。</p> 
 <p>在webmagic的设计中，除了Scheduler模块，其他的处理-从下载、解析到持久化，每个任务都是互相独立的，因此可以通过多个Spider共用一个Scheduler来进行扩展。排除去重的因素